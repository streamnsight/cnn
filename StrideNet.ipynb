{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StrideNet Convolutional Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/usr/local/lib/python3.7/site-packages/cv2.cpython-37m-darwin.so, 2): Library not loaded: /usr/local/opt/ilmbase/lib/libImath-2_2.23.dylib\n  Referenced from: /usr/local/Cellar/opencv/3.4.3/lib/libopencv_imgcodecs.3.4.dylib\n  Reason: image not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a656b4ac9f77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# to import / resize images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[0;31m# Open CV 3.1, on Mac OS brew install opencv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/usr/local/lib/python3.7/site-packages/cv2.cpython-37m-darwin.so, 2): Library not loaded: /usr/local/opt/ilmbase/lib/libImath-2_2.23.dylib\n  Referenced from: /usr/local/Cellar/opencv/3.4.3/lib/libopencv_imgcodecs.3.4.dylib\n  Reason: image not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to glob files\n",
    "import glob\n",
    "# to import / resize images\n",
    "import cv2 # Open CV 3.1, on Mac OS brew install opencv\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to print classification stats in a nice format\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# for dynamic graph rendering\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]\n",
    "\n",
    "cmaps = ['Spectral', 'coolwarm', 'bwr', 'seismic']\n",
    "cmap = 'bwr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA FROM: http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\n",
    "!wget http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncompress data\n",
    "!tar xvf 101_ObjectCategories.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = '101_ObjectCategories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pick 4 classes for a start\n",
    "class_names = ['airplanes', 'Faces', 'Leopards', 'Motorbikes']\n",
    "# we can try with more:\n",
    "# class_names = ['airplanes', 'Faces', 'Leopards', 'Motorbikes', 'bonsai', 'bass', 'beaver', 'binocular', 'butterfly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten output into 1 vector\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "width = 320\n",
    "height = 200\n",
    "depth = 3\n",
    "\n",
    "# The StrideNet model uses a 'trick' to reduce dimensionality while convoluting, by using stride > 1 and no padding\n",
    "# Stride=2 means the output is one every other pixel, the lack of padding reduces the overall image size\n",
    "# by 2 pixels each pass\n",
    "\n",
    "# StrideNet model\n",
    "model = nn.Sequential(\n",
    "    # input 3x96x96, output 16 channels, 45x45\n",
    "    nn.Conv2d(3, 16, kernel_size=7, stride=2, padding=0),\n",
    "    # input 16x45x45 output 32x45x45\n",
    "    nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    # input 32x45x45 output 32x23x23\n",
    "    nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(32),\n",
    "\n",
    "    nn.Dropout2d(p=0.25),\n",
    "\n",
    "    # input 32x23x23 output 32x23x23\n",
    "    nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    # input 32x23x23 output 64x12x12\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(64),\n",
    "    \n",
    "    nn.Dropout2d(p=0.25),\n",
    "\n",
    "    # input 64x12x12 output 128x12x12\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(128),\n",
    "    # input 128x12x12 output 128x6x6\n",
    "    nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm2d(128),\n",
    "    \n",
    "    nn.Dropout2d(p=0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    nn.Linear(4608, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(512, len(class_names)),\n",
    "    nn.Softmax(dim=1),\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather data and labels into useable structures\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(image_dir+'/*/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    label = file.split(\"/\")[-2]\n",
    "    if label not in class_names:\n",
    "        continue\n",
    "    img = cv2.imread(file)\n",
    "    img_t = cv2.resize(img, (96, 96))\n",
    "    data.append(img_t)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=5, ncols=5, figsize=(15, 15))\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        axs[i,j].imshow(data[np.random.randint(0, len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: colors are off because we're using a different color space, but it doesn't really matter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelBinarizer is used to transform text labels into vectors\n",
    "lb = LabelBinarizer()\n",
    "labelsb = lb.fit_transform(labels)\n",
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))\n",
    "print(labelsb[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale all our data (images encoded in RGB 24 bits -> [0-255] per pixel) to [0, 1]\n",
    "data = np.array(data, dtype=float) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to what PyTorch expects, i.e. batch first, then channels (colors), width, height\n",
    "data = data.swapaxes(1,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n",
    "# 2233 images, colors (R,G,B), 96x96 pixels each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labelsb, test_size=0.25, stratify=labelsb, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch Tensors to feed to the network\n",
    "trainX = torch.Tensor(trainX)\n",
    "trainY = torch.Tensor(trainY)\n",
    "testX = torch.Tensor(testX)\n",
    "testY = torch.Tensor(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    optimizer.zero_grad()\n",
    "    output = model(trainX)\n",
    "    loss = loss_fn(output, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(1)\n",
    "ax = fig.add_subplot(111)\n",
    "fig.show()\n",
    "\n",
    "while step < 50:\n",
    "    loss = train()\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    ax.plot(losses, 'b-')\n",
    "    ax.set_title(\"Step: {} | Loss: {}\".format(step, loss))\n",
    "    losses.append(loss)\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "torch.save(model.state_dict(), 'Stridenet-{}.pth'.format(datetime.now().strftime(\"%Y%m%d_%H%M%S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload a model:\n",
    "model.load_state_dict(torch.load('Stridenet-20190124_152007.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification_report prints a nice output of the performances of our model\n",
    "\n",
    "# detach().numpy() is necessary to detach the Tensor data and conver to numpy array\n",
    "# argmax gives us the highest probability for each \n",
    "print(classification_report(testY.detach().numpy().argmax(axis=1),  \n",
    "                            predictions.detach().numpy().argmax(axis=1),\n",
    "                            target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageIdx = 2 # 1,3 = Motobike, 8 = Face, 2 = airplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,3))\n",
    "plt.imshow(testX[imageIdx].detach().numpy().swapaxes(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY[imageIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[imageIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -> Prediction is 0.999 = 99.9% for airplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing 'hidden' layers\n",
    "\n",
    "#### Note, we're not visualizing the 'kernels' (values that were trained) per se, but the images that are generated through convolution by those kernels, i.e. the transformation an input image goes through when computed by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a wrapper model, which can extract layers as needed\n",
    "class SpyNet(nn.Module):\n",
    "    def __init__(self, trained_model, depth):\n",
    "        super(SpyNet, self).__init__()\n",
    "        self.features = nn.Sequential(*list(trained_model.eval())[:depth])\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at 2 images and how the model transforms them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageIdx1 = 3 # Motorbike\n",
    "imageIdx2 = 9 # Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(6,3))\n",
    "ax[0].imshow(testX[imageIdx1].detach().numpy().swapaxes(0,2))\n",
    "ax[1].imshow(testX[imageIdx2].detach().numpy().swapaxes(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_model = SpyNet(model, 28) # adjust the layer number value to check out a given layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spy_model(testX)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npo = output.detach().numpy()\n",
    "npo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some fancy rendering of the hidden layers\n",
    "# Output images from each kernel is displayed so that the 2 input images are shows on successive rows.\n",
    "# 1st row is 1st image, 2nd row is second image for the same kernels etc...\n",
    "\n",
    "import math\n",
    "m = npo.shape[1] # nb of kernels\n",
    "if len(npo.shape) > 2:\n",
    "    w = npo.shape[2] # size of image\n",
    "    c = min(m, int(math.ceil(np.sqrt(m)))) #250 / max(w, 24))) # nb columns\n",
    "    r = int(math.ceil(m / c)) # nb rows\n",
    "    index = np.array(range(0, r * c)).reshape(r, -1)\n",
    "    # figure some math to layout the images\n",
    "    fig, ax = plt.subplots(nrows=2*r, ncols=c, figsize=(15, 2 * (15 / c) * r))\n",
    "    for id in range(m):\n",
    "        ix = np.where(index==id)\n",
    "        ax[2*ix[0][0]  , ix[1][0]].imshow(npo[imageIdx1][id].swapaxes(0,1), cmap=cmap)\n",
    "        ax[2*ix[0][0]+1, ix[1][0]].imshow(npo[imageIdx2][id].swapaxes(0,1), cmap=cmap)\n",
    "else:\n",
    "    # reshape the output to look like some sort of image to visualize\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10, 3), dpi=100)\n",
    "    ax[0].imshow(npo[imageIdx1].reshape(-1, min(m, 64)))\n",
    "    ax[1].imshow(npo[imageIdx2].reshape(-1, min(m, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
