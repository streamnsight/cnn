{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Activation layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An activation layer is like a switch, which determines what Neuron needs to fire.\n",
    "\n",
    "The Activation layer is what brings non-linearity in the network.\n",
    "If the system was linear, there would always be a single composite transformation to represent the whole network, which defeats the purpose of deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = plt.imread('zebra.jpg')\n",
    "img2 = plt.imread('lion.jpg')\n",
    "gray1 = np.mean(img1[...,:3], -1)\n",
    "gray2 = np.mean(img2[...,:3], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize this to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray1 = (gray1 - 128.0) / 128.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Activation layers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step function\n",
    "\n",
    "As simple as it gets: the step function is a binary switch. It's 0 if the input is negative, and 1 if the output is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(-100,100, 1)) / 100.0\n",
    "y = np.append(np.zeros(100), np.ones(100), axis=0)\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.plot(x='x', y='y', figsize=(5,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue with a step function is that the model can easily get stuck: no matter the value in negative, it will output 0, on the positive side, not matter the value it will return 1, therefore there is no possible way to 'tune' the parameters to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = gray1.copy()\n",
    "np.clip(gray1, 0.0, 1.0, out=o)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 7), dpi=80)\n",
    "ax[0].imshow(gray1, cmap='gray')\n",
    "ax[1].imshow(o, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "The sigmoid function is very common: it provides a non-linear range that now allows the model to fit more easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    y = 1.0 / (1.0 + np.exp(-1.0 * x))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(range(-100, 100, 1)) / 10.0\n",
    "y = sigmoid(x)\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.plot(x='x', y='y', figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = sigmoid(gray1)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 7), dpi=80)\n",
    "ax[0].imshow(gray1, cmap='gray')\n",
    "ax[1].imshow(o, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arctangent\n",
    "\n",
    "Similar to the sigmoid, but with a different slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atan(x):\n",
    "    return np.arctan(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(range(-100, 100, 1)) / 10.0\n",
    "y = atan(X)\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.plot(x='x', y='y', figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = gray1.shape\n",
    "o = atan(gray1.reshape(-1)).reshape(h, w)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 7), dpi=80)\n",
    "ax[0].imshow(gray1, cmap='gray')\n",
    "ax[1].imshow(o, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU: Rectifier Linear Unit\n",
    "\n",
    "The standard ReLU is basically a function that is 0 for negative numbers, and y=x for positive numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    if x >= 0.0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(range(-100, 100, 1)) / 10.0\n",
    "y = np.vectorize(ReLU)(X)\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.plot(x='x', y='y', figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 7), dpi=80)\n",
    "ax[0].imshow(gray1, cmap='gray')\n",
    "ax[1].imshow(np.vectorize(ReLU)(gray1), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ReLU is effective, but like the step function, the optimizer may get stuck on negative numbers because the output is 0 no matter what the negative value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "The leaky ReLU solves the problems of the ReLU by 'leaking' through on the negative side, so that there is a much lower slope to the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeakyReLU(x, p):\n",
    "    if x >=0:\n",
    "        return x\n",
    "    else:\n",
    "        return x / p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(range(-100, 100, 1)) / 10.0\n",
    "y = np.vectorize(LeakyReLU)(X, 20.0)\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.plot(x='x', y='y', figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 7), dpi=80)\n",
    "ax[0].imshow(gray1, cmap='gray')\n",
    "ax[1].imshow(np.vectorize(LeakyReLU)(gray1, 20.0), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swish\n",
    "\n",
    "Swish has become popular at some point, but many other papers report worse results.\n",
    "http://aclweb.org/anthology/D18-1472\n",
    "\n",
    "So, something to try maybe, maybe not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(range(-100, 100, 1)) / 10.0\n",
    "y = [swish(x) for x in X]\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.plot(x='x', y='y', figsize=(5,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU6\n",
    "\n",
    "The idea of ReLU6 is that with a ReLU, the model may end up diverging to infinity because of the y=x part of the equation.\n",
    "ReLU6 limits y to 6 for x >= 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU6(x):\n",
    "    if x > 6:\n",
    "        return 6\n",
    "    if x >=0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(range(-100, 100, 1)) / 10.0\n",
    "y = [ReLU6(x) for x in X]\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.plot(x='x', y='y', figsize=(5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
